# Global-Video-Object-Segmentation-with-Spatial-Constraint-Module

## Abstract
We present a lightweight and efficient semi-supervised video object segmentation network based on the space-time memory framework. To some extent, our method solves the two difficulties encountered in traditional video object segmentation: one is that the single frame calculation time is too long, and the other is that the current frame’s segmentation should use more information from past frames. The algorithm uses a global context (GC) module to achieve high-performance, real-time segmentation. The GC module can effectively integrate multi-frame image information without increased memory and can process each frame in real time. Moreover, the prediction mask of the previous frame is helpful for the segmentation of the current frame, so we input it into a spatial constraints module (SCM), which constrains the areas of segments in the current frame. The SCM effectively alleviates mismatching of similar targets yet consumes few additional resources. We added a refinement module to the decoder to improve boundary segmentation. Our model achieves state-of-the-art results on various datasets, scoring 80.1 on YouTube-VOS 2018 and a J &F score of 78.0 on DAVIS 2017, while taking 0.05 seconds per frame on the DAVIS 2016 validation dataset.

## Pipeline
![图片2](https://user-images.githubusercontent.com/21287744/190839978-90be494e-1f38-4613-820b-78d05d290344.jpg)
Description: Past frames are input into the network for encoding and sent to a fixed-size global context module. The module’s content is updated automatically as frames advance. The encoder generates a set of attention vectors for the current frame to retrieve relevant information in the global context, to form global features. The encoder also generates local features. The global and local features are concatenated and passed to the constraint module, whose result is passed to the decoder to produce the segmentation result for the frame.
